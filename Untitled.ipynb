{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/IdoZehori/Credit_Score/blob/master/Credit%20score.ipynb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "datatrain1 = pd.read_csv(u\"dataset/train1.csv\", index_col=0)\n",
    "datatest1 = pd.read_csv(u\"dataset/test1.csv\")\n",
    "datatrain2 = pd.read_csv(u\"dataset/train2.csv\")\n",
    "datatest2 = pd.read_csv(u\"dataset/test2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatrain1[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatrain1.purpose = pd.CategoricalIndex(datatrain1.purpose)\n",
    "datatrain1[\"code\"] = datatrain1.purpose.cat.codes\n",
    "datatrain1[:6]\n",
    "# datatrain1.purpose.astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newDF = pd.DataFrame()\n",
    "newDF[\"purpose\"] = pd.CategoricalIndex(datatrain1.purpose)\n",
    "newDF[\"purpose\"] = newDF.purpose.cat.codes\n",
    "# newDF[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "newDF = pd.DataFrame()\n",
    "lb_make = LabelEncoder()\n",
    "newDF[\"purpose_code\"] = lb_make.fit_transform(datatrain1[\"purpose\"])\n",
    "lb_make = LabelEncoder()\n",
    "newDF[\"initial_list_status_code\"] = lb_make.fit_transform(datatrain1[\"initial_list_status\"])\n",
    "lb_make = LabelEncoder()\n",
    "newDF[\"verification_status_code\"] = lb_make.fit_transform(datatrain1[\"verification_status\"])\n",
    "lb_make = LabelEncoder()\n",
    "newDF[\"grade_code\"] = lb_make.fit_transform(datatrain1[\"grade\"])\n",
    "lb_make = LabelEncoder()\n",
    "newDF[\"home_ownership_code\"] = lb_make.fit_transform(datatrain1[\"home_ownership\"])\n",
    "newDF[\"int_rate\"] = datatrain1[\"int_rate\"]\n",
    "newDF[\"installment\"] = datatrain1[\"installment\"]\n",
    "newDF[\"annual_inc\"] = datatrain1[\"annual_inc\"]\n",
    "newDF[\"dti\"] = datatrain1[\"dti\"]\n",
    "newDF[\"revol_bal\"] = datatrain1[\"revol_bal\"]\n",
    "newDF[\"inq_last_12m\"] = datatrain1[\"inq_last_12m\"]\n",
    "newDF[\"delinq_2yrs\"] = datatrain1[\"delinq_2yrs\"]\n",
    "newDF[\"not_paid\"] = datatrain1[\"not_paid\"]\n",
    "newDF[\"log_inc\"] = datatrain1[\"log_inc\"]\n",
    "newDF[\"verified\"] = datatrain1[\"verified\"]\n",
    "newDF[\"grdCtoA\"] = datatrain1[\"grdCtoA\"]\n",
    "newDF[\"code\"] = datatrain1[\"code\"]\n",
    "\n",
    "\n",
    "newDF[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatrain1.int_rate.plot.box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatrain1.installment.plot.box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatrain1.annual_inc.plot.box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatrain1.dti.plot.box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatrain1.revol_bal.plot.box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatrain1.log_inc.plot.box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "imp_1 = Imputer(missing_values='Nan',strategy=\"median\",axis=0)\n",
    "imp_2 = Imputer(missing_values='Nan',strategy=\"median\",axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  add_freq ():\n",
    "    ncount = len(datatrain1)\n",
    "\n",
    "    ax2=ax.twinx()\n",
    "\n",
    "    ax2.yaxis.tick_left()\n",
    "    ax.yaxis.tick_right()\n",
    "\n",
    "    ax.yaxis.set_label_position('right')\n",
    "    ax2.yaxis.set_label_position('left')\n",
    "\n",
    "    ax2.set_ylabel('Frequency [%]')\n",
    "\n",
    "    for p in ax.patches:\n",
    "        x=p.get_bbox().get_points()[:,0]\n",
    "        y=p.get_bbox().get_points()[1,1]\n",
    "        ax.annotate('{:.1f}%'.format(100.*y/ncount), (x.mean(), y), \n",
    "                ha='center', va='bottom')\n",
    "\n",
    "    ax2.set_ylim(0,10)\n",
    "    ax2.grid(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.countplot(x = datatrain1.not_paid ,palette=\"Set3\")\n",
    "sns.set(font_scale=1.5)\n",
    "ax.set_ylim(top = 2400)\n",
    "ax.set_xlabel(' ')\n",
    "ax.set_ylabel(' ')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10,5)\n",
    "ax.set_ylim(top=2400)\n",
    "\n",
    "add_freq()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mad_based_outlier(points, thresh=3.5):\n",
    "    if len(points.shape) == 1:\n",
    "        points = points[:,None]\n",
    "    median = np.median(points, axis=0)\n",
    "    diff = np.sum((points - median)**2, axis=-1)\n",
    "    diff = np.sqrt(diff)\n",
    "    med_abs_deviation = np.median(diff)\n",
    "\n",
    "    modified_z_score = 0.6745 * diff / med_abs_deviation\n",
    "\n",
    "    return modified_z_score > thresh\n",
    "\n",
    "def percentile_based_outlier(data, threshold=95):\n",
    "    diff = (100 - threshold) / 2.0\n",
    "    (minval, maxval) = np.percentile(data, [diff, 100 - diff])\n",
    "    return ((data < minval) | (data > maxval))\n",
    "\n",
    "\n",
    "def std_div(data, threshold=3):\n",
    "    std = data.std()\n",
    "    mean = data.mean()\n",
    "    isOutlier = []\n",
    "    for val in data:\n",
    "        if val/std > threshold:\n",
    "            isOutlier.append(True)\n",
    "        else:\n",
    "            isOutlier.append(False)\n",
    "    return isOutlier\n",
    "\n",
    "def outlierVote(data):\n",
    "    x = percentile_based_outlier(data)\n",
    "    y = mad_based_outlier(data)\n",
    "    z = std_div(data)\n",
    "    temp = list(zip(data.index, x, y, z))\n",
    "    final = []\n",
    "    for i in range(len(temp)):\n",
    "        if temp[i].count(False) >= 2:\n",
    "            final.append(False)\n",
    "        else:\n",
    "            final.append(True)\n",
    "    return final\n",
    "\n",
    "def plotOutlier(x):\n",
    "    fig, axes = plt.subplots(nrows=4)\n",
    "    for ax, func in zip(axes, [percentile_based_outlier, mad_based_outlier, std_div, outlierVote]):\n",
    "        sns.distplot(x, ax=ax, rug=True, hist=False)\n",
    "        outliers = x[func(x)]\n",
    "        ax.plot(outliers, np.zeros_like(outliers), 'ro', clip_on=False)\n",
    "\n",
    "    kwargs = dict(y=0.95, x=0.05, ha='left', va='top', size=20)\n",
    "    axes[0].set_title('Percentile-based Outliers', **kwargs)\n",
    "    axes[1].set_title('MAD-based Outliers', **kwargs)\n",
    "    axes[2].set_title('STD-based Outliers', **kwargs)\n",
    "    axes[3].set_title('Majority vote based Outliers', **kwargs)\n",
    "    fig.suptitle('Comparing Outlier Tests with n={}'.format(len(x)), size=20)\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(15,10)\n",
    "    \n",
    "def plotOutlierFree(x):\n",
    "    fig, axes = plt.subplots(nrows=4)\n",
    "    nOutliers = []\n",
    "    for ax, func in zip(axes, [percentile_based_outlier, mad_based_outlier, std_div, outlierVote]):\n",
    "        tfOutlier = list(zip(x, func(x)))\n",
    "        nOutliers.append(len([index for (index, bol) in tfOutlier if bol == True]))\n",
    "        outlierFree = [index for (index, bol) in tfOutlier if bol == True]\n",
    "        sns.distplot(outlierFree, ax=ax, rug=True, hist=False)\n",
    "        \n",
    "    kwargs = dict(y=0.95, x=0.05, ha='left', va='top', size=15)\n",
    "    axes[0].set_title('Percentile-based Outliers, removed: {r}'.format(r=nOutliers[0]), **kwargs)\n",
    "    axes[1].set_title('MAD-based Outliers, removed: {r}'.format(r=nOutliers[1]), **kwargs)\n",
    "    axes[2].set_title('STD-based Outliers, removed: {r}'.format(r=nOutliers[2]), **kwargs)\n",
    "    axes[3].set_title('Majority vote based Outliers, removed: {r}'.format(r=nOutliers[3]), **kwargs)\n",
    "    fig.suptitle('Outlier Removed By Method with n={}'.format(len(x)), size=20)\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(15,10)\n",
    "\n",
    "def outlierRatio(data):\n",
    "    functions = [percentile_based_outlier, mad_based_outlier, std_div, outlierVote]\n",
    "    outlierDict = {}\n",
    "    for func in functions:\n",
    "        funcResult = func(data)\n",
    "        count = 0\n",
    "        for val in funcResult:\n",
    "            if val == True:\n",
    "                count += 1 \n",
    "        outlierDict[str(func)[10:].split()[0]] = [count, '{:.2f}%'.format((float(count)/len(data))*100)]\n",
    "    \n",
    "    return outlierDict\n",
    "\n",
    "def replaceOutlier(data, method = outlierVote, replace='median'):\n",
    "    '''replace: median (auto)\n",
    "                'minUpper' which is the upper bound of the outlier detection'''\n",
    "    vote = outlierVote(data)\n",
    "    x = pd.DataFrame(zip(data, vote), columns=['debt', 'outlier'])\n",
    "    if replace == 'median':\n",
    "        replace = x.debt.median()\n",
    "    elif replace == 'minUpper':\n",
    "        replace = min([val for (val, vote) in zip(data, vote) if vote == True])\n",
    "        if replace < data.mean():\n",
    "            return 'There are outliers lower than the sample mean'\n",
    "    debtNew = []\n",
    "    for i in range(x.shape[0]):\n",
    "        if x.iloc[i][1] == True:\n",
    "            debtNew.append(replace)\n",
    "        else:\n",
    "            debtNew.append(x.iloc[i][0])\n",
    "    \n",
    "    return debtNew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotOutlier(datatrain1.annual_inc.sample(1244))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotOutlierFree(datatrain1.annual_inc.sample(1244))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_transform = [ 'a', 'list', 'of', 'categorical', 'column', 'names' ]\n",
    "df_with_dummies = pd.get_dummies( columns = cols_to_transform )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LinearRegression\n",
    "# from sklearn.ensemble import GradientBoostingRegressor\n",
    "# from sklearn.ensemble import AdaBoostRegressor\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.neighbors import KNeighborsRegressor\n",
    "# from sklearn.grid_search import RandomizedSearchCV \n",
    "# from sklearn.cross_validation import cross_val_score\n",
    "# from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train  =  datatrain1[:1000]\n",
    "# test = datatrain1[1000:]\n",
    "                         \n",
    "# train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lmModlmMod  = LinearRegression(fit_intercept=True, normalize=True).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nan Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Train - Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newDF = newDF.dropna()\n",
    "X =  newDF.drop('not_paid', axis=1)\n",
    "y = newDF.not_paid\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "newDF.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sScaler = StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "\n",
    "xScaled = sScaler.fit_transform(X_train)\n",
    "\n",
    "forPca = pd.DataFrame(xScaled)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pcaMod = PCA(n_components=2)\n",
    "\n",
    "xPca = pcaMod.fit_transform(X)\n",
    "\n",
    "xPcaDataframe = pd.DataFrame(xPca, columns=['PC1', 'PC2'])\n",
    "\n",
    "xPcaDataframe['cat'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.lmplot(data = xPcaDataframe, x='PC2', y='PC1', hue='cat', size=10, aspect=20, fit_reg=False,\n",
    "               scatter_kws={'alpha': 0.3})\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.preprocessing import minmax_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xScaled  =  minmax_scale (X, feature_range=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldaMod = LinearDiscriminantAnalysis(solver='eigen', shrinkage='auto', priors=None, n_components=2, store_covariance=False, tol=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fittedLdaMod = ldaMod.fit(xScaled, y).transform(xScaled)\n",
    "ldaDf = pd.DataFrame(fittedLdaMod, columns=['one'])\n",
    "ldaDf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ax = sns.lmplot(data = ldaDf, x='one', y='two', hue='cat', size=30, aspect=5, fit_reg=False, scatter_kws={'alpha': 0.3})\n",
    "# fig = plt.gcf()\n",
    "# fig.set_size_inches(15, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  sklearn.manifold import TSNE\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsneMod = TSNE(n_components=2, perplexity=30.0, early_exaggeration=4.0, learning_rate=50., n_iter=500,\n",
    "               metric='euclidean', init='random', verbose=2, random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 7000\n",
    "X_data, X_none, y_data, y_none = train_test_split(X, y, test_size=(1-(n/float(len(datatrain1)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTsne = tsneMod.fit_transform(X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xTsneDataFrame  = pd.DataFrame(xTsne, columns=['one', 'two'])\n",
    "\n",
    "y_data.index = range(0,len(xTsneDataFrame))\n",
    "\n",
    "xTsneDataFrame['cat'] = y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.lmplot(data = xTsneDataFrame, x='one', y='two', hue='cat', size=70, aspect=5, fit_reg=False, palette='Set2',\n",
    "               scatter_kws={\"s\": 50, 'alpha': 0.5})\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make  Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knMod = KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2,\n",
    "                             metric='minkowski', metric_params=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glmMod = LogisticRegression(penalty='l1', dual=False, tol=0.0001, C=1.0, fit_intercept=True,\n",
    "                            intercept_scaling=1, class_weight=None, random_state=None, solver='liblinear', max_iter=100,\n",
    "                            multi_class='ovr', verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaMod = AdaBoostClassifier(base_estimator=None, n_estimators=200, learning_rate=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbMod = GradientBoostingClassifier(loss='deviance', learning_rate=0.1, n_estimators=200, subsample=1.0,\n",
    "                                   min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3,\n",
    "                                   init=None, random_state=None, max_features=None, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfMod = RandomForestClassifier(n_estimators=10, criterion='gini', max_depth=None, min_samples_split=2,\n",
    "                               min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto',\n",
    "                               max_leaf_nodes=None, bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cvDictGen(functions, scr, X_train=X_train, y_train=y_train, cv=3, verbose=1):\n",
    "    cvDict = {}\n",
    "    for func in functions:\n",
    "        cvScore = cross_val_score(func, X_train, y_train, cv=cv, verbose=verbose, scoring=scr)\n",
    "        cvDict[str(func).split('(')[0]] = [cvScore.mean(), cvScore.std()]\n",
    "    \n",
    "    return cvDict\n",
    "\n",
    "def cvDictNormalize(cvDict):\n",
    "    cvDictNormalized = {}\n",
    "    for key in list(cvDict.keys()):\n",
    "        for i in cvDict[key]:\n",
    "            cvDictNormalized[key] = ['{:0.2f}'.format((cvDict[key][0]/cvDict[cvDict.keys()[0]][0])),\n",
    "                                     '{:0.2f}'.format((cvDict[key][1]/cvDict[cvDict.keys()[0]][1]))]\n",
    "    return cvDictNormalized\n",
    "\n",
    "cvD = cvDictGen(functions=[knMod, glmMod, adaMod, gbMod, rfMod], scr='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvDictNormalize(cvD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
